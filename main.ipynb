{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a110c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anshumansinha/Desktop/GTN\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d51feef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "598d469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anshumansinha/Desktop/GTN\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d618061e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data1: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir data1\n",
    "! cd data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7440a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file=None, dataset='DBLP', epoch=2, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=3, norm='true', adaptive_lr='false')\n",
      "Namespace(file=None, dataset='DBLP', epoch=2, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=3, norm='true', adaptive_lr='false')\n",
      "2022-07-08 20:50:20.381109: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "Start of epoch 0\n",
      "in\n",
      "(1, 18405, 18405, 5)\n",
      "sw tf.Tensor(\n",
      "[[[[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]]\n",
      "\n",
      "\n",
      " [[[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]\n",
      "\n",
      "  [[0.1]]]], shape=(2, 5, 1, 1), dtype=float32)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/anshumansinha/Desktop/GTN/main.py\", line 112, in <module>\n",
      "    loss,y_train,Ws = model(A, node_features, train_node, train_target)\n",
      "  File \"/Users/anshumansinha/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/Users/anshumansinha/Desktop/GTN/model_tf_3.py\", line 99, in call\n",
      "    H, W = self.layers[i](A) #self.layers = nn.ModuleList(layers)\n",
      "  File \"/Users/anshumansinha/Desktop/GTN/model_tf_3.py\", line 142, in call\n",
      "    a = self.conv1(A)\n",
      "  File \"/Users/anshumansinha/Desktop/GTN/model_tf_3.py\", line 180, in call\n",
      "    A = tf.reduce_sum(A*(tf.nn.softmax(self.weight,1)), 1)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling layer \"gt_conv\" (type GTConv).\n",
      "\n",
      "Incompatible shapes: [1,18405,18405,5] vs. [2,5,1,1] [Op:Mul]\n",
      "\n",
      "Call arguments received:\n",
      "  â€¢ A=tf.Tensor(shape=(1, 18405, 18405, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "! python main.py --dataset DBLP --num_layers 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "067b29ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01f6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file='/Users/anshumansinha/Library/Jupyter/runtime/kernel-9f4d6b12-d929-4f0b-85cc-f9558f3a5dd4.json', dataset=None, epoch=40, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=2, norm='true', adaptive_lr='false')\n",
      "Namespace(file='/Users/anshumansinha/Library/Jupyter/runtime/kernel-9f4d6b12-d929-4f0b-85cc-f9558f3a5dd4.json', dataset=None, epoch=40, node_dim=64, num_channels=2, lr=0.005, weight_decay=0.001, num_layers=2, norm='true', adaptive_lr='false')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m norm \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mnorm\n\u001b[1;32m     47\u001b[0m adaptive_lr \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39madaptive_lr\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/node_features.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     50\u001b[0m     node_features \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/edges.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from model_tf_2 import GTN\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "from utils import f1_score\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--file', '-f', type=str)\n",
    "    parser.add_argument('--dataset', type=str,\n",
    "                        help='Dataset')\n",
    "    parser.add_argument('--epoch', type=int, default=40,\n",
    "                        help='Training Epochs')\n",
    "    parser.add_argument('--node_dim', type=int, default=64,\n",
    "                        help='Node dimension')\n",
    "    parser.add_argument('--num_channels', type=int, default=2,\n",
    "                        help='number of channels')\n",
    "    parser.add_argument('--lr', type=float, default=0.005,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.001,\n",
    "                        help='l2 reg')\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                        help='number of layer')\n",
    "    parser.add_argument('--norm', type=str, default='true',\n",
    "                        help='normalization')\n",
    "    parser.add_argument('--adaptive_lr', type=str, default='false',\n",
    "                        help='adaptive learning rate')\n",
    "\n",
    "    print(parser.parse_args())\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(args)\n",
    "    \n",
    "    epochs = args.epoch\n",
    "    node_dim = args.node_dim\n",
    "    num_channels = args.num_channels\n",
    "    lr = args.lr\n",
    "    weight_decay = args.weight_decay\n",
    "    num_layers = args.num_layers\n",
    "    norm = args.norm\n",
    "    adaptive_lr = args.adaptive_lr\n",
    "\n",
    "    with open('data/'+args.dataset+'/node_features.pkl','rb') as f:\n",
    "        node_features = pickle.load(f)\n",
    "    with open('data/'+args.dataset+'/edges.pkl','rb') as f:\n",
    "        edges = pickle.load(f)\n",
    "    with open('data/'+args.dataset+'/labels.pkl','rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "        \n",
    "    # creation of nodes, edges and labels in 3 separate matrix/ vectors!    \n",
    "    \n",
    "    num_nodes = edges[0].shape[0]\n",
    "\n",
    "    # A = Adjacency matrix \n",
    "    \n",
    "    for i,edge in enumerate(edges): # i goesthrough numbers [0,1,2,3...] and edge through edges.\n",
    "        if i ==0:\n",
    "             A = tf.expand_dims(tf.convert_to_tensor(edge.todense(), dtype= tf.float32), -1)\n",
    "        else:\n",
    "             A = tf.concat((A,tf.expand_dims(tf.convert_to_tensor(edge.todense(), dtype= tf.float32), -1)), dim =-1) \n",
    "    \n",
    "    A = tf.concat((A, tf.expand_dims(tf.convert_to_tensor(tf.eye(num_nodes), dtype= tf.float32), -1) ), dim=-1)\n",
    "    \n",
    "    node_features = tf.convert_to_tensor(node_features, dtype= tf.float32)\n",
    "    \n",
    "    train_node = tf.convert_to_tensor(np.array(labels[0])[:,0])\n",
    "    train_target = tf.convert_to_tensor(np.array(labels[0])[:,1])\n",
    "    \n",
    "    valid_node = tf.convert_to_tensor(np.array(labels[1])[:,0])\n",
    "    valid_target = tf.convert_to_tensor(np.array(labels[1])[:,1])\n",
    "    \n",
    "    test_node = tf.convert_to_tensor(np.array(labels[2])[:,0])\n",
    "    test_target = tf.convert_to_tensor(np.array(labels[2])[:,1])\n",
    "    \n",
    "    num_classes = tf.get_static_value(tf.reduce_max(train_target)) +1\n",
    "    # num_classes = tf.math.maximum(train_target).item()+1\n",
    "    \n",
    "    final_f1 = 0\n",
    "    \n",
    "    for l in tf.range(1):\n",
    "        \n",
    "        model = GTN(num_edge=A.shape[-1],\n",
    "                            num_channels=num_channels,\n",
    "                            w_in = node_features.shape[1],\n",
    "                            w_out = node_dim,\n",
    "                            num_class=num_classes,\n",
    "                            num_layers=num_layers,\n",
    "                            norm=norm)\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(lr=0.005, weight_decay=0.001)\n",
    "\n",
    "        # Train & Valid & Test\n",
    "        best_val_loss = 10000\n",
    "        best_test_loss = 10000\n",
    "        best_train_loss = 10000\n",
    "        best_train_f1 = 0\n",
    "        best_val_f1 = 0\n",
    "        best_test_f1 = 0\n",
    "        \n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            print(\"\\nStart of epoch %d\" % (i,))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "              loss,y_train,Ws = model(A, node_features, train_node, train_target)\n",
    "              train_f1 = torch.mean(f1_score(torch.argmax(y_train,dim=1), train_target, num_classes=num_classes)).cpu().numpy()\n",
    "              print('Train - Loss: {}, Macro_F1: {}'.format(loss.cpu().numpy(), train_f1))\n",
    "\n",
    "            grads = tape.gradient(loss, model.parameters())\n",
    "            optimizer.apply_gradients(zip(grads, model.parameters()))\n",
    "\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_loss = val_loss.cpu().numpy()\n",
    "                best_train_loss = loss.cpu().numpy()\n",
    "                best_train_f1 = train_f1\n",
    "                best_val_f1 = val_f1\n",
    "\n",
    "        print('---------------Best Results--------------------')\n",
    "        print('Train - Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "        print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "        final_f1 += best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834e83d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
